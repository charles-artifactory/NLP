{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd22b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'STFangsong'\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "MODEL_DIR = \"../model\"\n",
    "RESULT_DIR = \"../result\"\n",
    "DATA_DIR = \"../data\"\n",
    "\n",
    "Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(RESULT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(DATA_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed2e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup done.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "RAW_WIKI_DIR = Path(DATA_DIR) / \"output\"\n",
    "TOKENIZED_PATH = Path(DATA_DIR) / \"tokenized.txt\"\n",
    "VOCAB_PATH = Path(MODEL_DIR) / \"vocab.pkl\"          # word2id, id2word, counts\n",
    "COOC_NPZ = Path(MODEL_DIR) / \"cooc.npz\"             # co-occurrence triples (i, j, x_ij)\n",
    "VECTORS_TXT = Path(MODEL_DIR) / \"vectors.txt\"       # exported word + vectors\n",
    "EMB_TARGET_PT = Path(MODEL_DIR) / \"glove_W.pt\"\n",
    "EMB_CONTEXT_PT = Path(MODEL_DIR) / \"glove_C.pt\"\n",
    "EMB_MERGED_PT = Path(MODEL_DIR) / \"glove_merged.pt\"\n",
    "\n",
    "# Corpus / preprocessing\n",
    "SPLIT_SENT = True\n",
    "MAX_DOCS = None\n",
    "MAX_LINES_PER_FILE = None\n",
    "\n",
    "# Tokenization / vocab\n",
    "MIN_COUNT = 5\n",
    "MAX_VOCAB = 400_000\n",
    "KEEP_DIGITS = False\n",
    "LOWER_CASE = False\n",
    "RESERVED_TOKENS = [\"<unk>\"]\n",
    "\n",
    "# Co-occurrence\n",
    "WINDOW_SIZE = 10\n",
    "SYMMETRIC = True\n",
    "WEIGHT_BY_DISTANCE = True\n",
    "\n",
    "# GloVe training\n",
    "EMBED_DIM = 300\n",
    "BATCH_SIZE = 131072\n",
    "EPOCHS = 25\n",
    "LR = 0.05\n",
    "X_MAX = 100\n",
    "ALPHA = 0.75\n",
    "SEED = 2025\n",
    "\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "def save_fig(path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path)\n",
    "    print(f\"Saved figure to: {path}\")\n",
    "\n",
    "\n",
    "def normalize_text(txt: str) -> str:\n",
    "    \"\"\"Basic cleaning for Chinese Wikipedia text.\"\"\"\n",
    "    if txt is None:\n",
    "        return \"\"\n",
    "    if LOWER_CASE:\n",
    "        txt = txt.lower()\n",
    "    txt = re.sub(r\"[\\x00-\\x08\\x0B-\\x1F\\x7F]\", \" \", txt)\n",
    "    return txt\n",
    "\n",
    "\n",
    "def sentence_split(txt: str):\n",
    "    if not SPLIT_SENT:\n",
    "        return [txt]\n",
    "    sents = re.split(r\"[。！？!?；;\\n\\r]+\", txt)\n",
    "    sents = [s.strip() for s in sents if s.strip()]\n",
    "    return sents\n",
    "\n",
    "\n",
    "cc = None\n",
    "\n",
    "print(\"Setup done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef6de279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing docs: 0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.413 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Tokenizing docs: 1907228it [21:04, 1508.21it/s, docs=1.9e+6, sents=1.42e+7, tokens=2.38e+8]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization finished. Docs=1,907,228, Sents=14,209,318, Tokens=237,753,400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def iter_wiki_json_lines(raw_dir: Path, max_docs=None, max_lines_per_file=None):\n",
    "    \"\"\"Yield raw text from WikiExtractor JSONL files under raw_dir.\"\"\"\n",
    "    count_docs = 0\n",
    "    for root, _dirs, files in os.walk(raw_dir):\n",
    "        files = sorted(files)\n",
    "        for fname in files:\n",
    "            if not fname.startswith(\"wiki_\"):\n",
    "                continue\n",
    "            fpath = Path(root) / fname\n",
    "            with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    if max_lines_per_file is not None and i >= max_lines_per_file:\n",
    "                        break\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    text = obj.get(\"text\", \"\") or \"\"\n",
    "                    title = obj.get(\"title\", \"\") or \"\"\n",
    "                    if title:\n",
    "                        text = title + \"。\" + text\n",
    "                    yield text\n",
    "                    count_docs += 1\n",
    "                    if max_docs is not None and count_docs >= max_docs:\n",
    "                        return\n",
    "\n",
    "\n",
    "def tokenize_to_file(raw_dir: Path, out_path: Path, keep_digits=False, cc_converter=None, max_docs=None, max_lines_per_file=None):\n",
    "    \"\"\"Stream JSONL -> tokenize -> write one sentence per line (space-separated tokens).\"\"\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    total_tokens = 0\n",
    "    total_sents = 0\n",
    "    total_docs = 0\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        pbar = tqdm(iter_wiki_json_lines(raw_dir, max_docs, max_lines_per_file), desc=\"Tokenizing docs\")\n",
    "        for raw in pbar:\n",
    "            total_docs += 1\n",
    "            txt = normalize_text(raw)\n",
    "            if cc_converter:\n",
    "                txt = cc_converter.convert(txt)\n",
    "            sents = sentence_split(txt)\n",
    "            for s in sents:\n",
    "                toks = [t.strip() for t in jieba.lcut(s) if t.strip()]\n",
    "                if not keep_digits:\n",
    "                    toks = [t for t in toks if not t.isdigit()]\n",
    "                if len(toks) == 0:\n",
    "                    continue\n",
    "                out_f.write(\" \".join(toks) + \"\\n\")\n",
    "                total_tokens += len(toks)\n",
    "                total_sents += 1\n",
    "            if total_docs % 5000 == 0:\n",
    "                pbar.set_postfix({\"docs\": total_docs, \"sents\": total_sents, \"tokens\": total_tokens})\n",
    "\n",
    "    print(f\"Tokenization finished. Docs={total_docs:,}, Sents={total_sents:,}, Tokens={total_tokens:,}\")\n",
    "    return dict(docs=total_docs, sents=total_sents, tokens=total_tokens)\n",
    "\n",
    "\n",
    "stats = tokenize_to_file(RAW_WIKI_DIR, TOKENIZED_PATH, keep_digits=KEEP_DIGITS, cc_converter=cc,\n",
    "                         max_docs=MAX_DOCS, max_lines_per_file=MAX_LINES_PER_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d28ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting vocab: 14209318it [00:31, 457434.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab built. Size=400,001 (min_count=5, max_vocab=400000)\n",
      "Saved figure to: ../result/zipf_curve.png\n"
     ]
    }
   ],
   "source": [
    "def build_vocab_from_tokenized(path: Path, min_count=5, max_vocab=400_000, reserved_tokens=None):\n",
    "    \"\"\"Count tokens -> prune -> build word2id/id2word.\"\"\"\n",
    "    cnt = Counter()\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\"Counting vocab\"):\n",
    "            toks = line.strip().split()\n",
    "            cnt.update(toks)\n",
    "\n",
    "    items = [(w, c) for w, c in cnt.items() if c >= min_count]\n",
    "    items.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if max_vocab is not None:\n",
    "        items = items[:max_vocab]\n",
    "\n",
    "    word2id = {}\n",
    "    id2word = []\n",
    "    counts = []\n",
    "\n",
    "    if reserved_tokens:\n",
    "        for t in reserved_tokens:\n",
    "            word2id[t] = len(id2word)\n",
    "            id2word.append(t)\n",
    "            counts.append(0)\n",
    "\n",
    "    for w, c in items:\n",
    "        if w in word2id:\n",
    "            continue\n",
    "        word2id[w] = len(id2word)\n",
    "        id2word.append(w)\n",
    "        counts.append(c)\n",
    "\n",
    "    vocab = {\"word2id\": word2id, \"id2word\": id2word, \"counts\": np.array(counts, dtype=np.int64)}\n",
    "    with open(VOCAB_PATH, \"wb\") as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    print(f\"Vocab built. Size={len(id2word):,} (min_count={min_count}, max_vocab={max_vocab})\")\n",
    "    return vocab, cnt\n",
    "\n",
    "\n",
    "vocab, full_counter = build_vocab_from_tokenized(TOKENIZED_PATH, MIN_COUNT, MAX_VOCAB, RESERVED_TOKENS)\n",
    "\n",
    "freqs = np.array(sorted(full_counter.values(), reverse=True), dtype=np.float64)\n",
    "ranks = np.arange(1, len(freqs)+1, dtype=np.float64)\n",
    "\n",
    "plt.figure()\n",
    "plt.loglog(ranks, freqs)\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Token Frequency (Zipf-like)\")\n",
    "save_fig(Path(RESULT_DIR) / \"zipf_curve.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca1736fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VOCAB_PATH, \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "word2id = vocab[\"word2id\"]\n",
    "id2word = vocab[\"id2word\"]\n",
    "counts = vocab[\"counts\"]\n",
    "vocab_size = len(id2word)\n",
    "unk_id = word2id.get(\"<unk>\", None)\n",
    "\n",
    "\n",
    "def line_to_ids(line: str):\n",
    "    ids = []\n",
    "    for t in line.strip().split():\n",
    "        idx = word2id.get(t, unk_id)\n",
    "        if idx is not None:\n",
    "            ids.append(idx)\n",
    "    return ids\n",
    "\n",
    "\n",
    "def build_cooccurrence(tokenized_path: Path, window_size=10, symmetric=True, weight_by_distance=True):\n",
    "    \"\"\"\n",
    "    Build co-occurrence dict: (i, j) -> x_ij.\n",
    "    WARNING: This can be large for full Wikipedia; ensure enough RAM.\n",
    "    \"\"\"\n",
    "    cooc = defaultdict(float)\n",
    "    with open(tokenized_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\"Building co-occurrence\"):\n",
    "            ids = line_to_ids(line)\n",
    "            n = len(ids)\n",
    "            for center in range(n):\n",
    "                w_i = ids[center]\n",
    "                start = max(0, center - window_size)\n",
    "                end = min(n, center + window_size + 1)\n",
    "                for ctx in range(start, end):\n",
    "                    if ctx == center:\n",
    "                        continue\n",
    "                    w_j = ids[ctx]\n",
    "                    dist = abs(ctx - center)\n",
    "                    if dist == 0:\n",
    "                        continue\n",
    "                    weight = (1.0 / dist) if weight_by_distance else 1.0\n",
    "                    cooc[(w_i, w_j)] += weight\n",
    "                    if symmetric:\n",
    "                        cooc[(w_j, w_i)] += weight\n",
    "    if len(cooc) == 0:\n",
    "        raise RuntimeError(\"Empty co-occurrence matrix. Check preprocessing.\")\n",
    "    i_idx = np.fromiter((k[0] for k in cooc.keys()), dtype=np.int32, count=len(cooc))\n",
    "    j_idx = np.fromiter((k[1] for k in cooc.keys()), dtype=np.int32, count=len(cooc))\n",
    "    x_val = np.fromiter(cooc.values(), dtype=np.float32, count=len(cooc))\n",
    "    np.savez_compressed(COOC_NPZ, i=i_idx, j=j_idx, x=x_val, vocab_size=np.int32(vocab_size))\n",
    "    print(f\"Co-occurrence saved: {COOC_NPZ}, nnz={len(cooc):,}\")\n",
    "    cooc.clear()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245ffe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence: 14209318it [44:14, 5353.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence saved: ../model/cooc.npz, nnz=415,615,849\n"
     ]
    }
   ],
   "source": [
    "build_cooccurrence(TOKENIZED_PATH, WINDOW_SIZE, SYMMETRIC, WEIGHT_BY_DISTANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd64d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoocDataset(Dataset):\n",
    "    def __init__(self, npz_path: Path):\n",
    "        dat = np.load(npz_path)\n",
    "        self.i = dat[\"i\"].astype(np.int64)\n",
    "        self.j = dat[\"j\"].astype(np.int64)\n",
    "        self.x = dat[\"x\"].astype(np.float32)\n",
    "        self.vocab_size = int(dat[\"vocab_size\"])\n",
    "        assert len(self.i) == len(self.j) == len(self.x)\n",
    "        print(f\"Loaded cooc triples: {len(self.x):,}, vocab_size={self.vocab_size:,}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.i[idx], self.j[idx], self.x[idx]\n",
    "\n",
    "\n",
    "class GloVeModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, dim: int):\n",
    "        super().__init__()\n",
    "        self.w = nn.Embedding(vocab_size, dim)   # target\n",
    "        self.c = nn.Embedding(vocab_size, dim)   # context\n",
    "        self.bw = nn.Embedding(vocab_size, 1)    # bias for w\n",
    "        self.bc = nn.Embedding(vocab_size, 1)    # bias for c\n",
    "        init_range = 0.5 / dim\n",
    "        nn.init.uniform_(self.w.weight, a=-init_range, b=init_range)\n",
    "        nn.init.uniform_(self.c.weight, a=-init_range, b=init_range)\n",
    "        nn.init.zeros_(self.bw.weight)\n",
    "        nn.init.zeros_(self.bc.weight)\n",
    "\n",
    "    def forward(self, i_idx, j_idx):\n",
    "        w_i = self.w(i_idx)          # (B, D)\n",
    "        c_j = self.c(j_idx)          # (B, D)\n",
    "        bw_i = self.bw(i_idx).squeeze(-1)  # (B,)\n",
    "        bc_j = self.bc(j_idx).squeeze(-1)  # (B,)\n",
    "        dot = (w_i * c_j).sum(dim=1)       # (B,)\n",
    "        return dot + bw_i + bc_j\n",
    "\n",
    "\n",
    "def glove_loss(pred, x_ij, x_max=100.0, alpha=0.75):\n",
    "    \"\"\"\n",
    "    pred = w_i^T c_j + b_i + b_j\n",
    "    target = log(x_ij)\n",
    "    weight = (x_ij / x_max)^alpha if x_ij < x_max else 1\n",
    "    \"\"\"\n",
    "    log_x = torch.log(x_ij)\n",
    "    w = torch.where(x_ij < x_max, (x_ij / x_max) ** alpha, torch.ones_like(x_ij))\n",
    "    loss = w * (pred - log_x) ** 2\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89321c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cooc triples: 415,615,849, vocab_size=400,001\n"
     ]
    }
   ],
   "source": [
    "dataset = CoocDataset(COOC_NPZ)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False, num_workers=0)\n",
    "\n",
    "model = GloVeModel(dataset.vocab_size, EMBED_DIM).to(device)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee54a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cooc triples: 415,615,849, vocab_size=400,001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|██████████| 3171/3171 [14:07<00:00,  3.74it/s, loss=0.0809]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.080469, time=847.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|██████████| 3171/3171 [14:22<00:00,  3.67it/s, loss=0.0438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss=0.043797, time=862.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|██████████| 3171/3171 [14:44<00:00,  3.59it/s, loss=0.0353] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: loss=0.035293, time=884.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|██████████| 3171/3171 [14:32<00:00,  3.64it/s, loss=0.0311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: loss=0.031140, time=872.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|██████████| 3171/3171 [14:37<00:00,  3.61it/s, loss=0.0286] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: loss=0.028622, time=878.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|██████████| 3171/3171 [14:38<00:00,  3.61it/s, loss=0.0269]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: loss=0.026886, time=878.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|██████████| 3171/3171 [14:41<00:00,  3.60it/s, loss=0.0256] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: loss=0.025595, time=882.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|██████████| 3171/3171 [14:33<00:00,  3.63it/s, loss=0.0246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: loss=0.024585, time=873.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|██████████| 3171/3171 [14:40<00:00,  3.60it/s, loss=0.0238] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: loss=0.023766, time=880.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|██████████| 3171/3171 [14:41<00:00,  3.60it/s, loss=0.0231]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: loss=0.023084, time=881.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|██████████| 3171/3171 [14:40<00:00,  3.60it/s, loss=0.0225] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: loss=0.022505, time=880.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|██████████| 3171/3171 [14:35<00:00,  3.62it/s, loss=0.0220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: loss=0.022004, time=875.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|██████████| 3171/3171 [14:40<00:00,  3.60it/s, loss=0.0216] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: loss=0.021567, time=880.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|██████████| 3171/3171 [14:26<00:00,  3.66it/s, loss=0.0212]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: loss=0.021179, time=866.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|██████████| 3171/3171 [14:27<00:00,  3.65it/s, loss=0.0208] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: loss=0.020834, time=867.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|██████████| 3171/3171 [14:13<00:00,  3.72it/s, loss=0.0205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: loss=0.020523, time=853.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|██████████| 3171/3171 [14:23<00:00,  3.67it/s, loss=0.0202] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: loss=0.020241, time=863.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|██████████| 3171/3171 [14:18<00:00,  3.69it/s, loss=0.0200]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: loss=0.019984, time=858.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|██████████| 3171/3171 [14:23<00:00,  3.67it/s, loss=0.0197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: loss=0.019749, time=863.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|██████████| 3171/3171 [14:18<00:00,  3.69it/s, loss=0.0195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: loss=0.019532, time=858.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|██████████| 3171/3171 [14:19<00:00,  3.69it/s, loss=0.0193] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: loss=0.019332, time=859.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|██████████| 3171/3171 [14:16<00:00,  3.70it/s, loss=0.0191]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: loss=0.019146, time=856.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|██████████| 3171/3171 [14:21<00:00,  3.68it/s, loss=0.0190] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: loss=0.018973, time=861.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|██████████| 3171/3171 [14:10<00:00,  3.73it/s, loss=0.0188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: loss=0.018812, time=850.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|██████████| 3171/3171 [14:14<00:00,  3.71it/s, loss=0.0187] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: loss=0.018661, time=854.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to: ../model/glove_W.pt, ../model/glove_C.pt\n",
      "Saved figure to: ../result/glove_training_loss.png\n"
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    n_batches = 0\n",
    "    t0 = time.time()\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    for (i_idx, j_idx, x_ij) in pbar:\n",
    "        i_idx = i_idx.to(device)\n",
    "        j_idx = j_idx.to(device)\n",
    "        x_ij = x_ij.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        pred = model(i_idx, j_idx)\n",
    "        loss = glove_loss(pred, x_ij, X_MAX, ALPHA)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running += loss.item()\n",
    "        n_batches += 1\n",
    "        if n_batches % 100 == 0:\n",
    "            pbar.set_postfix({\"loss\": f\"{running/n_batches:.4f}\"})\n",
    "\n",
    "    epoch_loss = running / max(1, n_batches)\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    dt = time.time() - t0\n",
    "    print(f\"Epoch {epoch}: loss={epoch_loss:.6f}, time={dt:.1f}s\")\n",
    "\n",
    "torch.save(model.w.weight.detach().cpu(), EMB_TARGET_PT)\n",
    "torch.save(model.c.weight.detach().cpu(), EMB_CONTEXT_PT)\n",
    "torch.save(model.state_dict(), Path(MODEL_DIR) / \"glove_model_state.pt\")\n",
    "print(f\"Saved embeddings to: {EMB_TARGET_PT}, {EMB_CONTEXT_PT}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1, len(epoch_losses)+1), epoch_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"GloVe Training Loss\")\n",
    "save_fig(Path(RESULT_DIR) / \"glove_training_loss.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36b32c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged embedding saved to: ../model/glove_merged.pt\n"
     ]
    }
   ],
   "source": [
    "W = torch.load(EMB_TARGET_PT, map_location=\"cpu\")\n",
    "C = torch.load(EMB_CONTEXT_PT, map_location=\"cpu\")\n",
    "merged = (W + C) / 2.0\n",
    "torch.save(merged, EMB_MERGED_PT)\n",
    "print(f\"Merged embedding saved to: {EMB_MERGED_PT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d406260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported vectors to: ../model/vectors.txt\n"
     ]
    }
   ],
   "source": [
    "with open(VECTORS_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, word in enumerate(id2word):\n",
    "        vec = merged[idx].numpy()\n",
    "        f.write(word + \" \" + \" \".join(f\"{x:.6f}\" for x in vec) + \"\\n\")\n",
    "print(f\"Exported vectors to: {VECTORS_TXT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c311e8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to '中国':\n",
      "  大陆\t0.7101\n",
      "  中國\t0.5929\n",
      "  当时\t0.5713\n",
      "  中华人民共和国\t0.5712\n",
      "  历史\t0.5645\n",
      "  内地\t0.5614\n",
      "  于\t0.5612\n",
      "  同时\t0.5545\n",
      "  成为\t0.5539\n",
      "  这是\t0.5506\n",
      "Nearest to '北京':\n",
      "  上海\t0.6605\n",
      "  南京\t0.5643\n",
      "  天津\t0.5554\n",
      "  北京市\t0.5396\n",
      "  北平\t0.5353\n",
      "  广州\t0.5205\n",
      "  中国\t0.5110\n",
      "  杭州\t0.5095\n",
      "  赴\t0.5094\n",
      "  成都\t0.5063\n",
      "Nearest to '美国':\n",
      "  英国\t0.6713\n",
      "  美國\t0.5845\n",
      "  华盛顿\t0.5583\n",
      "  澳大利亚\t0.5573\n",
      "  加拿大\t0.5558\n",
      "  成为\t0.5507\n",
      "  国家\t0.5483\n",
      "  并且\t0.5468\n",
      "  当时\t0.5459\n",
      "  他们\t0.5421\n",
      "Nearest to '数学':\n",
      "  物理学\t0.6340\n",
      "  计算机科学\t0.6060\n",
      "  应用\t0.6019\n",
      "  理论\t0.5845\n",
      "  哲学\t0.5787\n",
      "  化学\t0.5786\n",
      "  科学\t0.5747\n",
      "  物理\t0.5665\n",
      "  计算\t0.5561\n",
      "  领域\t0.5485\n",
      "Nearest to '哲学':\n",
      "  神学\t0.6678\n",
      "  社会学\t0.6188\n",
      "  社会科学\t0.6060\n",
      "  文学\t0.5892\n",
      "  心理学\t0.5843\n",
      "  科学\t0.5838\n",
      "  马克思主义\t0.5837\n",
      "  经济学\t0.5808\n",
      "  政治学\t0.5803\n",
      "  数学\t0.5787\n",
      "Nearest to '上海':\n",
      "  北京\t0.6605\n",
      "  天津\t0.6413\n",
      "  广州\t0.6138\n",
      "  南京\t0.5871\n",
      "  杭州\t0.5772\n",
      "  上海市\t0.5665\n",
      "  武汉\t0.5358\n",
      "  廣州\t0.5341\n",
      "  重庆\t0.5333\n",
      "  深圳\t0.5246\n"
     ]
    }
   ],
   "source": [
    "E = merged.numpy().astype(np.float32)\n",
    "norm = np.linalg.norm(E, axis=1, keepdims=True) + 1e-12\n",
    "E_norm = E / norm\n",
    "\n",
    "w2i = word2id\n",
    "i2w = id2word\n",
    "\n",
    "\n",
    "def most_similar(word, topn=10):\n",
    "    \"\"\"Return top-N nearest neighbors by cosine similarity.\"\"\"\n",
    "    if word not in w2i:\n",
    "        return []\n",
    "    idx = w2i[word]\n",
    "    q = E_norm[idx]\n",
    "    sims = E_norm @ q\n",
    "    sims[idx] = -1.0\n",
    "    top_idx = np.argpartition(-sims, range(topn))[:topn]\n",
    "    top_idx = top_idx[np.argsort(-sims[top_idx])]\n",
    "    result = [(i2w[i], float(sims[i])) for i in top_idx]\n",
    "    return result\n",
    "\n",
    "\n",
    "test_words = [\"中国\", \"北京\", \"美国\", \"数学\", \"哲学\", \"上海\"]\n",
    "for w in test_words:\n",
    "    if w in w2i:\n",
    "        print(f\"Nearest to '{w}':\")\n",
    "        for ww, sc in most_similar(w, topn=10):\n",
    "            print(f\"  {ww}\\t{sc:.4f}\")\n",
    "    else:\n",
    "        print(f\"'{w}' not in vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e89609e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figure to: ../result/glove_tsne.png\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "def plot_tsne(words, out_path: Path):\n",
    "    indices = [w2i[w] for w in words if w in w2i]\n",
    "    if len(indices) < 2:\n",
    "        print(\"Not enough words found in vocab for t-SNE.\")\n",
    "        return\n",
    "    X = E[indices]\n",
    "    tsne = TSNE(n_components=2, init=\"random\", learning_rate=\"auto\", perplexity=min(30, len(indices)-1), n_iter_without_progress=2000)\n",
    "    X2 = tsne.fit_transform(X)\n",
    "    plt.figure()\n",
    "    plt.scatter(X2[:, 0], X2[:, 1])\n",
    "    for i, w in enumerate([w for w in words if w in w2i]):\n",
    "        plt.text(X2[i, 0], X2[i, 1], w)\n",
    "    plt.title(\"t-SNE of Selected Words\")\n",
    "    save_fig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "example_words = [\"中国\", \"北京\", \"上海\", \"美国\", \"纽约\", \"数学\", \"代数\", \"几何\", \"物理\", \"化学\", \"哲学\", \"逻辑\", \"计算机\", \"算法\", \"数据\"]\n",
    "plot_tsne(example_words, Path(RESULT_DIR) / \"glove_tsne.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cd22b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "Path(\"../model\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"../result\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"../data\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a1861bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集句子数量: 86918\n",
      "测试集句子数量: 3985\n",
      "训练集示例: ['“', '人', '们', '常', '说', '生', '活', '是', '一', '部', '教', '科', '书', '，', '而', '血', '与', '火', '的', '战', '争', '更', '是', '不', '可', '多', '得', '的', '教', '科', '书', '，', '她', '确', '实', '是', '名', '副', '其', '实', '的', '‘', '我', '的', '大', '学', '’', '。']\n",
      "对应标签: ['S', 'B', 'E', 'S', 'S', 'B', 'E', 'S', 'S', 'S', 'B', 'M', 'E', 'S', 'S', 'S', 'S', 'S', 'S', 'B', 'E', 'S', 'S', 'B', 'M', 'M', 'E', 'S', 'B', 'M', 'E', 'S', 'S', 'B', 'E', 'S', 'B', 'M', 'M', 'E', 'S', 'S', 'S', 'S', 'B', 'E', 'S', 'S']\n"
     ]
    }
   ],
   "source": [
    "def load_data(file_path):\n",
    "    sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                words = [word for word in line.split() if word.strip()]\n",
    "                if words:\n",
    "                    sentences.append(words)\n",
    "    return sentences\n",
    "\n",
    "def words_to_chars_labels(sentences):\n",
    "    char_sequences = []\n",
    "    label_sequences = []\n",
    "    \n",
    "    for words in sentences:\n",
    "        chars = []\n",
    "        labels = []\n",
    "        \n",
    "        for word in words:\n",
    "            word = word.strip()\n",
    "            if len(word) == 1:\n",
    "                chars.append(word)\n",
    "                labels.append('S')  # Single\n",
    "            elif len(word) > 1:\n",
    "                chars.append(word[0])\n",
    "                labels.append('B')  # Begin\n",
    "                for char in word[1:-1]:\n",
    "                    chars.append(char)\n",
    "                    labels.append('M')  # Middle\n",
    "                chars.append(word[-1])\n",
    "                labels.append('E')  # End\n",
    "        \n",
    "        if chars:  # 只添加非空序列\n",
    "            char_sequences.append(chars)\n",
    "            label_sequences.append(labels)\n",
    "    \n",
    "    return char_sequences, label_sequences\n",
    "\n",
    "train_sentences = load_data(\"../data/icwb2-data/training/msr_training.utf8\")\n",
    "test_sentences = load_data(\"../data/icwb2-data/testing/msr_test.utf8\")\n",
    "\n",
    "train_chars, train_labels = words_to_chars_labels(train_sentences)\n",
    "test_chars, test_labels = words_to_chars_labels(test_sentences)\n",
    "\n",
    "print(f\"训练集句子数量: {len(train_chars)}\")\n",
    "print(f\"测试集句子数量: {len(test_chars)}\")\n",
    "print(f\"训练集示例: {train_chars[0]}\")\n",
    "print(f\"对应标签: {train_labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc270f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词典大小: 88119\n",
      "最大词长: 48\n",
      "测试文本: 我爱北京天安门\n",
      "正向最大匹配: ['我', '爱', '北京', '天安门']\n",
      "逆向最大匹配: ['我', '爱', '北京', '天安门']\n",
      "双向最大匹配: ['我', '爱', '北京', '天安门']\n"
     ]
    }
   ],
   "source": [
    "class MaximumMatching:\n",
    "    \"\"\"最大匹配分词算法\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word_dict = set()\n",
    "        self.max_len = 0\n",
    "    \n",
    "    def build_dict(self, sentences):\n",
    "        \"\"\"从训练数据构建词典\"\"\"\n",
    "        for words in sentences:\n",
    "            for word in words:\n",
    "                word = word.strip()\n",
    "                if word:\n",
    "                    self.word_dict.add(word)\n",
    "                    self.max_len = max(self.max_len, len(word))\n",
    "        \n",
    "        print(f\"词典大小: {len(self.word_dict)}\")\n",
    "        print(f\"最大词长: {self.max_len}\")\n",
    "    \n",
    "    def forward_max_matching(self, text):\n",
    "        \"\"\"正向最大匹配\"\"\"\n",
    "        result = []\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(text):\n",
    "            # 尝试最长匹配\n",
    "            matched = False\n",
    "            for length in range(min(self.max_len, len(text) - i), 0, -1):\n",
    "                word = text[i:i+length]\n",
    "                if word in self.word_dict:\n",
    "                    result.append(word)\n",
    "                    i += length\n",
    "                    matched = True\n",
    "                    break\n",
    "            \n",
    "            if not matched:\n",
    "                # 如果没有匹配，将单个字符作为词\n",
    "                result.append(text[i])\n",
    "                i += 1\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def backward_max_matching(self, text):\n",
    "        \"\"\"逆向最大匹配\"\"\"\n",
    "        result = []\n",
    "        i = len(text)\n",
    "        \n",
    "        while i > 0:\n",
    "            matched = False\n",
    "            for length in range(min(self.max_len, i), 0, -1):\n",
    "                word = text[i-length:i]\n",
    "                if word in self.word_dict:\n",
    "                    result.insert(0, word)\n",
    "                    i -= length\n",
    "                    matched = True\n",
    "                    break\n",
    "            \n",
    "            if not matched:\n",
    "                result.insert(0, text[i-1])\n",
    "                i -= 1\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def bidirectional_matching(self, text):\n",
    "        \"\"\"双向最大匹配\"\"\"\n",
    "        forward_result = self.forward_max_matching(text)\n",
    "        backward_result = self.backward_max_matching(text)\n",
    "        \n",
    "        # 选择词数更少的结果，如果词数相同，选择单字词更少的\n",
    "        if len(forward_result) < len(backward_result):\n",
    "            return forward_result\n",
    "        elif len(forward_result) > len(backward_result):\n",
    "            return backward_result\n",
    "        else:\n",
    "            # 词数相同，比较单字词数量\n",
    "            forward_single = sum(1 for word in forward_result if len(word) == 1)\n",
    "            backward_single = sum(1 for word in backward_result if len(word) == 1)\n",
    "            \n",
    "            if forward_single <= backward_single:\n",
    "                return forward_result\n",
    "            else:\n",
    "                return backward_result\n",
    "\n",
    "# 训练最大匹配模型\n",
    "mm_model = MaximumMatching()\n",
    "mm_model.build_dict(train_sentences)\n",
    "\n",
    "# 测试最大匹配算法\n",
    "test_text = \"我爱北京天安门\"\n",
    "print(f\"测试文本: {test_text}\")\n",
    "print(f\"正向最大匹配: {mm_model.forward_max_matching(test_text)}\")\n",
    "print(f\"逆向最大匹配: {mm_model.backward_max_matching(test_text)}\")\n",
    "print(f\"双向最大匹配: {mm_model.bidirectional_matching(test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1857ff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM-CRF模型定义完成\n"
     ]
    }
   ],
   "source": [
    "class ChineseSegmentationDataset(Dataset):\n",
    "    \"\"\"中文分词数据集\"\"\"\n",
    "    \n",
    "    def __init__(self, char_sequences, label_sequences, char2idx, label2idx):\n",
    "        self.char_sequences = char_sequences\n",
    "        self.label_sequences = label_sequences\n",
    "        self.char2idx = char2idx\n",
    "        self.label2idx = label2idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.char_sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chars = self.char_sequences[idx]\n",
    "        labels = self.label_sequences[idx]\n",
    "        \n",
    "        # 转换为索引\n",
    "        char_indices = [self.char2idx.get(char, self.char2idx['<UNK>']) for char in chars]\n",
    "        label_indices = [self.label2idx[label] for label in labels]\n",
    "        \n",
    "        return torch.tensor(char_indices), torch.tensor(label_indices)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"批处理函数\"\"\"\n",
    "    chars, labels = zip(*batch)\n",
    "    \n",
    "    # 获取最大长度\n",
    "    max_len = max(len(seq) for seq in chars)\n",
    "    \n",
    "    # 填充序列\n",
    "    padded_chars = []\n",
    "    padded_labels = []\n",
    "    lengths = []\n",
    "    \n",
    "    for char_seq, label_seq in zip(chars, labels):\n",
    "        length = len(char_seq)\n",
    "        lengths.append(length)\n",
    "        \n",
    "        # 填充到最大长度\n",
    "        padded_char = torch.cat([char_seq, torch.zeros(max_len - length, dtype=torch.long)])\n",
    "        padded_label = torch.cat([label_seq, torch.zeros(max_len - length, dtype=torch.long)])\n",
    "        \n",
    "        padded_chars.append(padded_char)\n",
    "        padded_labels.append(padded_label)\n",
    "    \n",
    "    return torch.stack(padded_chars), torch.stack(padded_labels), torch.tensor(lengths)\n",
    "\n",
    "class BiLSTMCRF(nn.Module):\n",
    "    \"\"\"基于BiLSTM-CRF的中文分词模型\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_tags, dropout=0.1):\n",
    "        super(BiLSTMCRF, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_tags = num_tags\n",
    "        \n",
    "        # 字符嵌入层\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # BiLSTM层\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, \n",
    "                           num_layers=2, bidirectional=True, \n",
    "                           dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 输出层\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, num_tags)\n",
    "        \n",
    "        # CRF转移矩阵\n",
    "        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags))\n",
    "        \n",
    "        # 初始化参数\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"初始化模型参数\"\"\"\n",
    "        nn.init.xavier_uniform_(self.char_embeddings.weight)\n",
    "        nn.init.xavier_uniform_(self.hidden2tag.weight)\n",
    "        nn.init.constant_(self.hidden2tag.bias, 0)\n",
    "    \n",
    "    def forward(self, chars, lengths):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        batch_size, seq_len = chars.size()\n",
    "        \n",
    "        # 字符嵌入\n",
    "        embeddings = self.char_embeddings(chars)\n",
    "        \n",
    "        # 打包序列\n",
    "        packed_embeddings = nn.utils.rnn.pack_padded_sequence(\n",
    "            embeddings, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # BiLSTM\n",
    "        packed_lstm_out, _ = self.lstm(packed_embeddings)\n",
    "        \n",
    "        # 解包序列\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(packed_lstm_out, batch_first=True)\n",
    "        \n",
    "        # Dropout\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # 标签分数\n",
    "        tag_scores = self.hidden2tag(lstm_out)\n",
    "        \n",
    "        return tag_scores\n",
    "    \n",
    "    def crf_loss(self, emissions, tags, lengths):\n",
    "        \"\"\"计算CRF损失\"\"\"\n",
    "        batch_size, seq_len, num_tags = emissions.size()\n",
    "        \n",
    "        # 前向算法计算分配函数\n",
    "        def forward_algorithm(emissions, transitions):\n",
    "            batch_size, seq_len, num_tags = emissions.size()\n",
    "            alpha = torch.full((batch_size, num_tags), -10000.0, device=emissions.device)\n",
    "            alpha[:, 0] = emissions[:, 0, 0]  # 开始标签\n",
    "            \n",
    "            for i in range(1, seq_len):\n",
    "                alpha_t = alpha.unsqueeze(2) + transitions.unsqueeze(0) + emissions[:, i].unsqueeze(1)\n",
    "                alpha = torch.logsumexp(alpha_t, dim=1)\n",
    "            \n",
    "            return torch.logsumexp(alpha, dim=1)\n",
    "        \n",
    "        # 计算真实路径分数\n",
    "        def score_sentence(emissions, tags, transitions):\n",
    "            batch_size, seq_len = tags.size()\n",
    "            score = torch.zeros(batch_size, device=emissions.device)\n",
    "            \n",
    "            # 第一个标签的发射分数\n",
    "            score += emissions[range(batch_size), 0, tags[:, 0]]\n",
    "            \n",
    "            # 转移分数和发射分数\n",
    "            for i in range(1, seq_len):\n",
    "                score += transitions[tags[:, i-1], tags[:, i]]\n",
    "                score += emissions[range(batch_size), i, tags[:, i]]\n",
    "            \n",
    "            return score\n",
    "        \n",
    "        # 计算前向分数\n",
    "        forward_score = forward_algorithm(emissions, self.transitions)\n",
    "        \n",
    "        # 计算真实路径分数\n",
    "        gold_score = score_sentence(emissions, tags, self.transitions)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = forward_score - gold_score\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "print(\"BiLSTM-CRF模型定义完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4252cf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字符词汇表大小: 5169\n",
      "标签数量: 5\n",
      "训练批次数: 2717\n",
      "测试批次数: 125\n"
     ]
    }
   ],
   "source": [
    "# 构建字符词汇表\n",
    "def build_vocab(char_sequences):\n",
    "    char_counter = Counter()\n",
    "    for chars in char_sequences:\n",
    "        char_counter.update(chars)\n",
    "    \n",
    "    # 创建字符到索引的映射\n",
    "    char2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for char, count in char_counter.most_common():\n",
    "        if count >= 1:  # 最小频次阈值\n",
    "            char2idx[char] = len(char2idx)\n",
    "    \n",
    "    idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "    return char2idx, idx2char\n",
    "\n",
    "# 构建标签词汇表\n",
    "label2idx = {'<PAD>': 0, 'B': 1, 'M': 2, 'E': 3, 'S': 4}\n",
    "idx2label = {idx: label for label, idx in label2idx.items()}\n",
    "\n",
    "# 构建字符词汇表\n",
    "char2idx, idx2char = build_vocab(train_chars)\n",
    "\n",
    "print(f\"字符词汇表大小: {len(char2idx)}\")\n",
    "print(f\"标签数量: {len(label2idx)}\")\n",
    "\n",
    "# 创建数据集\n",
    "train_dataset = ChineseSegmentationDataset(train_chars, train_labels, char2idx, label2idx)\n",
    "test_dataset = ChineseSegmentationDataset(test_chars, test_labels, char2idx, label2idx)\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"训练批次数: {len(train_loader)}\")\n",
    "print(f\"测试批次数: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "609ed4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练BiLSTM模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 255/2717 [04:03<39:13,  1.05it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 81\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m开始训练BiLSTM模型...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# 训练\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# 评估\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate_simple(model, test_loader, device)\n",
      "Cell \u001b[0;32mIn[16], line 31\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcrf_loss(emissions, labels, lengths)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/NNDL/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/NNDL/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/NNDL/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 模型超参数\n",
    "vocab_size = len(char2idx)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_tags = len(label2idx)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "\n",
    "# 初始化模型\n",
    "model = BiLSTMCRF(vocab_size, embedding_dim, hidden_dim, num_tags).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练函数\n",
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for chars, labels, lengths in tqdm(train_loader, desc=\"Training\"):\n",
    "        chars = chars.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        emissions = model(chars, lengths)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = model.crf_loss(emissions, labels, lengths)\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# 简化的评估函数（不使用CRF解码）\n",
    "def evaluate_simple(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for chars, labels, lengths in test_loader:\n",
    "            chars = chars.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            emissions = model(chars, lengths)\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = model.crf_loss(emissions, labels, lengths)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # 简单预测（取最大值）\n",
    "            predictions = torch.argmax(emissions, dim=-1)\n",
    "            \n",
    "            # 计算准确率（只考虑有效长度）\n",
    "            for i, length in enumerate(lengths):\n",
    "                pred = predictions[i, :length]\n",
    "                true = labels[i, :length]\n",
    "                correct += (pred == true).sum().item()\n",
    "                total += length.item()\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# 训练模型\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(\"开始训练BiLSTM模型...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    \n",
    "    # 评估\n",
    "    val_loss, val_acc = evaluate_simple(model, test_loader, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'Train Loss: {train_loss:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}')\n",
    "        print(f'Val Accuracy: {val_acc:.4f}')\n",
    "        print('-' * 50)\n",
    "\n",
    "# 保存模型\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'char2idx': char2idx,\n",
    "    'idx2char': idx2char,\n",
    "    'label2idx': label2idx,\n",
    "    'idx2label': idx2label,\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_dim': embedding_dim,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'num_tags': num_tags\n",
    "}, '../model/bilstm_segmentation.pth')\n",
    "\n",
    "print(\"模型训练完成并已保存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6fcad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练曲线\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(val_accuracies, label='Val Accuracy', color='green')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(range(len(train_losses)), train_losses, 'b-', alpha=0.7, label='Train Loss')\n",
    "plt.plot(range(len(val_losses)), val_losses, 'r-', alpha=0.7, label='Val Loss')\n",
    "plt.fill_between(range(len(train_losses)), train_losses, alpha=0.3)\n",
    "plt.fill_between(range(len(val_losses)), val_losses, alpha=0.3)\n",
    "plt.title('Loss Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../result/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"最终验证准确率: {val_accuracies[-1]:.4f}\")\n",
    "print(f\"最终训练损失: {train_losses[-1]:.4f}\")\n",
    "print(f\"最终验证损失: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e7878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilstm_segment(model, text, char2idx, idx2label, device):\n",
    "    \"\"\"使用BiLSTM模型进行分词\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 转换为字符索引\n",
    "    char_indices = [char2idx.get(char, char2idx['<UNK>']) for char in text]\n",
    "    chars_tensor = torch.tensor([char_indices]).to(device)\n",
    "    lengths = torch.tensor([len(char_indices)])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 前向传播\n",
    "        emissions = model(chars_tensor, lengths)\n",
    "        \n",
    "        # 简单预测（取最大值，实际应用中可以使用维特比算法）\n",
    "        predictions = torch.argmax(emissions, dim=-1)\n",
    "        pred_labels = [idx2label[pred.item()] for pred in predictions[0]]\n",
    "    \n",
    "    # 根据标签进行分词\n",
    "    words = []\n",
    "    current_word = \"\"\n",
    "    \n",
    "    for char, label in zip(text, pred_labels):\n",
    "        if label == 'B':  # 词的开始\n",
    "            if current_word:\n",
    "                words.append(current_word)\n",
    "            current_word = char\n",
    "        elif label == 'M':  # 词的中间\n",
    "            current_word += char\n",
    "        elif label == 'E':  # 词的结束\n",
    "            current_word += char\n",
    "            words.append(current_word)\n",
    "            current_word = \"\"\n",
    "        elif label == 'S':  # 单字词\n",
    "            if current_word:\n",
    "                words.append(current_word)\n",
    "            words.append(char)\n",
    "            current_word = \"\"\n",
    "    \n",
    "    if current_word:\n",
    "        words.append(current_word)\n",
    "    \n",
    "    return words\n",
    "\n",
    "def compare_segmentation_methods(text):\n",
    "    \"\"\"比较不同分词方法的结果\"\"\"\n",
    "    print(f\"原文: {text}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 最大匹配算法\n",
    "    mm_result = mm_model.bidirectional_matching(text)\n",
    "    print(f\"最大匹配算法: {' / '.join(mm_result)}\")\n",
    "    \n",
    "    # BiLSTM算法\n",
    "    bilstm_result = bilstm_segment(model, text, char2idx, idx2label, device)\n",
    "    print(f\"BiLSTM算法: {' / '.join(bilstm_result)}\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    return mm_result, bilstm_result\n",
    "\n",
    "# 测试不同的文本\n",
    "test_texts = [\n",
    "    \"我爱北京天安门\",\n",
    "    \"机器学习是人工智能的重要分支\",\n",
    "    \"今天天气很好\",\n",
    "    \"自然语言处理技术发展迅速\",\n",
    "    \"北京大学是著名的高等学府\"\n",
    "]\n",
    "\n",
    "print(\"分词结果比较:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = {}\n",
    "for text in test_texts:\n",
    "    mm_result, bilstm_result = compare_segmentation_methods(text)\n",
    "    results[text] = {\n",
    "        'max_matching': mm_result,\n",
    "        'bilstm': bilstm_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae69be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存分词结果\n",
    "import json\n",
    "\n",
    "# 保存详细结果\n",
    "detailed_results = {\n",
    "    'training_info': {\n",
    "        'epochs': num_epochs,\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'final_val_loss': val_losses[-1],\n",
    "        'final_val_accuracy': val_accuracies[-1],\n",
    "        'vocab_size': vocab_size,\n",
    "        'model_parameters': sum(p.numel() for p in model.parameters())\n",
    "    },\n",
    "    'segmentation_results': results\n",
    "}\n",
    "\n",
    "with open('../result/segmentation_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(detailed_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 保存训练历史\n",
    "training_history = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accuracies': val_accuracies\n",
    "}\n",
    "\n",
    "with open('../result/training_history.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "\n",
    "# 创建结果总结表格\n",
    "summary_data = []\n",
    "for text, result in results.items():\n",
    "    summary_data.append({\n",
    "        '原文': text,\n",
    "        '最大匹配': ' / '.join(result['max_matching']),\n",
    "        'BiLSTM': ' / '.join(result['bilstm']),\n",
    "        '最大匹配词数': len(result['max_matching']),\n",
    "        'BiLSTM词数': len(result['bilstm'])\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv('../result/segmentation_comparison.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"实验总结:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"1. 数据集: MSR中文分词数据集（示例）\")\n",
    "print(f\"2. 训练样本数: {len(train_chars)}\")\n",
    "print(f\"3. 测试样本数: {len(test_chars)}\")\n",
    "print(f\"4. 字符词汇表大小: {vocab_size}\")\n",
    "print(f\"5. 模型参数数量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"6. 最终验证准确率: {val_accuracies[-1]:.4f}\")\n",
    "print(f\"7. 训练轮数: {num_epochs}\")\n",
    "print(f\"8. 使用设备: {device}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n分词方法比较:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n所有结果已保存到 ../result/ 目录\")\n",
    "print(f\"模型已保存到 ../model/ 目录\")\n",
    "print(\"\\n实验完成!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
